{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "14MV5l_e2SuZY1pwhi9UhX3yMcG2T6VD5",
      "authorship_tag": "ABX9TyMzx64rYB6202wvRZVNYQHx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpfcabral/datastructure/blob/main/project1/DCA0209_Project_Unit_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLIBV13r4j8u",
        "outputId": "eb5dcfcb-3369-4b76-ccba-94ceb9c67f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/UFRN/Engenharia de Computação/Estrutura de Dados II/archive.zip\n"
          ]
        }
      ],
      "source": [
        "!unzip -n \"/content/drive/MyDrive/UFRN/Engenharia de Computação/Estrutura de Dados II/archive.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "shqCk2VE9CqD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando classe com estruturas e algoritmos de melhor performance"
      ],
      "metadata": {
        "id": "hoXsXAfPmiZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/the-reddit-climate-change-dataset-comments.csv\"\n",
        "\n",
        "class RedditDataset:\n",
        "  def __init__(self, csv_path):\n",
        "    self.data = None\n",
        "\n",
        "    with open(csv_path) as csv_file:\n",
        "      reader = csv.reader(csv_file)\n",
        "      rows = list(reader)\n",
        "    \n",
        "    self.header = rows[0]\n",
        "    self.rows = rows[1:]\n",
        "\n",
        "    self.sentiment_dict = {}\n",
        "    for r in self.rows:\n",
        "      self.sentiment_dict[int(r[-1])] = r\n",
        "\n",
        "    self.data = {}\n",
        "    for r in self.rows:\n",
        "      self.data[r[1]] = r\n",
        "\n",
        "  def find_element_fast(self, id):\n",
        "    return self.data[id] if id in self.data.keys() else -1\n",
        "\n",
        "  def find_element(self, id):\n",
        "    for r in self.rows:\n",
        "      if r[1] == id:\n",
        "        return r\n",
        "    return -1\n",
        "\n",
        "  def find_messages_sentiment(self, inf=-1, sup=1):\n",
        "    message_list = []\n",
        "    for r in self.rows:\n",
        "      sentiment = float(r[8]) if r[8] != '' else 0.0\n",
        "      if sentiment > inf and sentiment < sup:\n",
        "        message_list.append(r[7])\n",
        "    return message_list\n",
        "  \n",
        "  def check_sum(self, soma):\n",
        "    for i in range(len(self.rows)-1):\n",
        "      for j in range(len(self.rows)-1):\n",
        "        if int(self.rows[i][-1]) + int(self.rows[j][-1]) == soma:\n",
        "          return self.rows[i], self.rows[j]\n",
        "    return -1\n",
        "\n",
        "\n",
        "  def check_sum_fast(self, soma):\n",
        "    for sentiment in self.sentiment_dict.keys():\n",
        "      res = sentiment - soma\n",
        "      if res in self.sentiment_dict.keys():\n",
        "        return self.sentiment_dict[sentiment], self.sentiment_dict[res]\n",
        "    return -1\n",
        "\n",
        "reddit_dataset = RedditDataset(data_path)"
      ],
      "metadata": {
        "id": "XtvJ5csh8OM2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisando busca por id"
      ],
      "metadata": {
        "id": "Js97-87SrFN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = [x[1] for x in reddit_dataset.rows]\n",
        "ids_selected = [ids[random.randint(0, len(ids))] for _ in range(500)]"
      ],
      "metadata": {
        "id": "amgEyGPho1RC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_time_slow = 0\n",
        "for id in ids_selected:\n",
        "  start = time.time()\n",
        "  reddit_dataset.find_element(id)\n",
        "  end = time.time()\n",
        "  total_time_slow += (end - start)\n",
        "\n",
        "print(total_time_slow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwElQEAPrCLo",
        "outputId": "14bd17d3-e0f7-4d51-a3e8-47e7472c9d05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "146.33182501792908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_time_fast = 0\n",
        "for id in ids_selected:\n",
        "  start = time.time()\n",
        "  reddit_dataset.find_element_fast(id)\n",
        "  end = time.time()\n",
        "  total_time_fast += (end - start)\n",
        "\n",
        "print(total_time_fast)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB4v2KNgr-gF",
        "outputId": "e6b63839-ad08-4362-ddd2-e8cba7286f3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009725093841552734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encontrando elementos entre limites"
      ],
      "metadata": {
        "id": "W_KYOP1l6NWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(reddit_dataset.find_messages_sentiment(-0.2, 0.1))"
      ],
      "metadata": {
        "id": "4XvWL-3zsE9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ca1069-581b-418b-e5fb-e43a884b0b02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "659445"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checando soma de scores"
      ],
      "metadata": {
        "id": "erDM5fi8PWx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [random.randint(0,200) for _ in range(10000)]"
      ],
      "metadata": {
        "id": "xzWzdAKAN6MA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_time_slow = 0\n",
        "for score in scores:\n",
        "  start = time.time()\n",
        "  reddit_dataset.check_sum(score)\n",
        "  end = time.time()\n",
        "  total_time_slow += (end - start)\n",
        "\n",
        "print(total_time_slow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tqsrTGtRqPJ",
        "outputId": "0e6026dd-cd12-4ba9-d9e9-2206805de034"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44.123188734054565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_time_fast = 0\n",
        "for score in scores:\n",
        "  start = time.time()\n",
        "  reddit_dataset.check_sum_fast(score)\n",
        "  end = time.time()\n",
        "  total_time_fast += (end - start)\n",
        "\n",
        "print(total_time_fast)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaNlPb7FR0XI",
        "outputId": "b4a1ba0a-9535-4108-cd3d-a61d92f71dbb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008278846740722656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes"
      ],
      "metadata": {
        "id": "aaUo4IbvSgjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytest pytest-sugar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mb8MA5ySgQQ",
        "outputId": "0ee315ba-c8c2-4a06-f158-eb87b3453aba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: pytest-sugar in /usr/local/lib/python3.7/dist-packages (0.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (22.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.14.0)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.7/dist-packages (from pytest-sugar) (21.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pytest-sugar) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.1->pytest-sugar) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file reddit_dataset_test.py\n",
        "import pytest\n",
        "import csv\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "data_path = \"/content/the-reddit-climate-change-dataset-comments.csv\"\n",
        "\n",
        "class RedditDataset:\n",
        "  def __init__(self, csv_path):\n",
        "    self.data = None\n",
        "\n",
        "    with open(csv_path) as csv_file:\n",
        "      reader = csv.reader(csv_file)\n",
        "      rows = list(reader)\n",
        "    \n",
        "    self.header = rows[0]\n",
        "    self.rows = rows[1:]\n",
        "\n",
        "    self.sentiment_dict = {}\n",
        "    for r in self.rows:\n",
        "      self.sentiment_dict[int(r[-1])] = r\n",
        "\n",
        "    self.data = {}\n",
        "    for r in self.rows:\n",
        "      self.data[r[1]] = r\n",
        "\n",
        "  def find_element_fast(self, id):\n",
        "    return self.data[id] if id in self.data.keys() else -1\n",
        "\n",
        "  def find_element(self, id):\n",
        "    for r in self.rows:\n",
        "      if r[1] == id:\n",
        "        return r\n",
        "    return -1\n",
        "\n",
        "  def find_messages_sentiment(self, inf=-1, sup=1):\n",
        "    message_list = []\n",
        "    for r in self.rows:\n",
        "      sentiment = float(r[8]) if r[8] != '' else 0.0\n",
        "      if sentiment > inf and sentiment < sup:\n",
        "        message_list.append(r[7])\n",
        "    return message_list\n",
        "  \n",
        "  def check_sum(self, soma):\n",
        "    for i in range(len(self.rows)-1):\n",
        "      for j in range(len(self.rows)-1):\n",
        "        if int(self.rows[i][-1]) + int(self.rows[j][-1]) == soma:\n",
        "          return self.rows[i], self.rows[j]\n",
        "    return -1\n",
        "\n",
        "\n",
        "  def check_sum_fast(self, soma):\n",
        "    for sentiment in self.sentiment_dict.keys():\n",
        "      res = sentiment - soma\n",
        "      if res in self.sentiment_dict.keys():\n",
        "        return self.sentiment_dict[sentiment], self.sentiment_dict[res]\n",
        "    return -1\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def data():\n",
        "    return RedditDataset(data_path) \n",
        "\n",
        "def test_find_element(data):\n",
        "  assert data.find_element('imlddn9') != -1\n",
        "\n",
        "def test_find_element_error(data):\n",
        "  assert data.find_element('imlddn9123456') == -1\n",
        "\n",
        "def test_find_element_fast(data):\n",
        "  assert data.find_element_fast('imlddn9') != -1\n",
        "\n",
        "def test_find_element_fast_error(data):\n",
        "  assert data.find_element_fast('imlddn9123456') == -1\n",
        "\n",
        "def test_find_messages_sentiment(data):\n",
        "  assert data.find_messages_sentiment(-0.1, 0.1) is not []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmexqpO9R5w4",
        "outputId": "ba2f32a2-ebdc-4c3a-8758-2eb891e2bb42"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reddit_dataset_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest /content/reddit_dataset_test.py -vv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3XLqg6sU-p5",
        "outputId": "76f9dced-6298-419c-a04b-86d0c57c7db9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.7.14, pytest 3.6.4, pytest-sugar 0.9.5)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.5\n",
            "\n",
            " \u001b[36mreddit_dataset_test.py\u001b[0m::test_find_element\u001b[0m \u001b[32m✓\u001b[0m                      \u001b[32m20% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█        \u001b[0m\n",
            " \u001b[36mreddit_dataset_test.py\u001b[0m::test_find_element_error\u001b[0m \u001b[32m✓\u001b[0m                \u001b[32m40% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█      \u001b[0m\n",
            " \u001b[36mreddit_dataset_test.py\u001b[0m::test_find_element_fast\u001b[0m \u001b[32m✓\u001b[0m                 \u001b[32m60% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█    \u001b[0m\n",
            " \u001b[36mreddit_dataset_test.py\u001b[0m::test_find_element_fast_error\u001b[0m \u001b[32m✓\u001b[0m           \u001b[32m80% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█  \u001b[0m\n",
            " \u001b[36mreddit_dataset_test.py\u001b[0m::test_find_messages_sentiment\u001b[0m \u001b[32m✓\u001b[0m          \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
            "\n",
            "Results (110.29s):\n",
            "\u001b[32m       5 passed\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1B694t2vVCwZ"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}